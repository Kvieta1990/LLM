{"text": "Scope: Data access\nCategory: Data Flow\nQ: Where can I download my collected data at SNS instruments?\nA: There are multiple options, as listed below,\n\n- Go to `https://nova.ornl.gov/`, log in with UCAMS/XCAMS account. Go to `Powder Diffraction` and launch the `PDiffVis` tool. With the tool, you can visualize the data and download them straightforwardly.\n\n- Go to `https://oncat.ornl.gov`, log in with UCAMS/XCAMS account, click on `Browse`, select the instrument, go to `Experiments`, select the IPTS, click on `Download` to go to the data download page. The option on the top will be to download data from the web.\n\n- Following the ONCAT instructions above, once located in the data download page, we can see other options for data download in the page, including using `SSH`, `FileZilla` or `CyberDuck`."}
{"text": "Scope: Data access\nCategory: Data Flow\nQ: How do I download my data from SNS?\nA: There are multiple options, as listed below,\n\n- Go to `https://nova.ornl.gov/`, log in with UCAMS/XCAMS account. Go to `Powder Diffraction` and launch the `PDiffVis` tool. With the tool, you can visualize the data and download them straightforwardly.\n\n- Go to `https://oncat.ornl.gov`, log in with UCAMS/XCAMS account, click on `Browse`, select the instrument, go to `Experiments`, select the IPTS, click on `Download` to go to the data download page. The option on the top will be to download data from the web.\n\n- Following the ONCAT instructions above, once located in the data download page, we can see other options for data download in the page, including using `SSH`, `FileZilla` or `CyberDuck`."}
{"text": "Scope: Data access\nCategory: Data Flow\nQ: What are the options for downloading SNS data?\nA: There are multiple options, as listed below,\n\n- Go to `https://nova.ornl.gov/`, log in with UCAMS/XCAMS account. Go to `Powder Diffraction` and launch the `PDiffVis` tool. With the tool, you can visualize the data and download them straightforwardly.\n\n- Go to `https://oncat.ornl.gov`, log in with UCAMS/XCAMS account, click on `Browse`, select the instrument, go to `Experiments`, select the IPTS, click on `Download` to go to the data download page. The option on the top will be to download data from the web.\n\n- Following the ONCAT instructions above, once located in the data download page, we can see other options for data download in the page, including using `SSH`, `FileZilla` or `CyberDuck`."}
{"text": "Scope: Data access\nCategory: Data Flow\nQ: I need to download my experiment data from SNS.\nA: There are multiple options, as listed below,\n\n- Go to `https://nova.ornl.gov/`, log in with UCAMS/XCAMS account. Go to `Powder Diffraction` and launch the `PDiffVis` tool. With the tool, you can visualize the data and download them straightforwardly.\n\n- Go to `https://oncat.ornl.gov`, log in with UCAMS/XCAMS account, click on `Browse`, select the instrument, go to `Experiments`, select the IPTS, click on `Download` to go to the data download page. The option on the top will be to download data from the web.\n\n- Following the ONCAT instructions above, once located in the data download page, we can see other options for data download in the page, including using `SSH`, `FileZilla` or `CyberDuck`."}
{"text": "Scope: Data access\nCategory: Data Flow\nQ: How can I access my collected data at SNS?\nA: There are multiple options, as listed below,\n\n- Go to `https://nova.ornl.gov/`, log in with UCAMS/XCAMS account. Go to `Powder Diffraction` and launch the `PDiffVis` tool. With the tool, you can visualize the data and download them straightforwardly.\n\n- Go to `https://oncat.ornl.gov`, log in with UCAMS/XCAMS account, click on `Browse`, select the instrument, go to `Experiments`, select the IPTS, click on `Download` to go to the data download page. The option on the top will be to download data from the web.\n\n- Following the ONCAT instructions above, once located in the data download page, we can see other options for data download in the page, including using `SSH`, `FileZilla` or `CyberDuck`."}
{"text": "Scope: Data access\nCategory: Data Flow\nQ: Tell me about data download options at SNS.\nA: There are multiple options, as listed below,\n\n- Go to `https://nova.ornl.gov/`, log in with UCAMS/XCAMS account. Go to `Powder Diffraction` and launch the `PDiffVis` tool. With the tool, you can visualize the data and download them straightforwardly.\n\n- Go to `https://oncat.ornl.gov`, log in with UCAMS/XCAMS account, click on `Browse`, select the instrument, go to `Experiments`, select the IPTS, click on `Download` to go to the data download page. The option on the top will be to download data from the web.\n\n- Following the ONCAT instructions above, once located in the data download page, we can see other options for data download in the page, including using `SSH`, `FileZilla` or `CyberDuck`."}
{"text": "Scope: Data access\nCategory: Data Flow\nQ: I want to retrieve my SNS data.\nA: There are multiple options, as listed below,\n\n- Go to `https://nova.ornl.gov/`, log in with UCAMS/XCAMS account. Go to `Powder Diffraction` and launch the `PDiffVis` tool. With the tool, you can visualize the data and download them straightforwardly.\n\n- Go to `https://oncat.ornl.gov`, log in with UCAMS/XCAMS account, click on `Browse`, select the instrument, go to `Experiments`, select the IPTS, click on `Download` to go to the data download page. The option on the top will be to download data from the web.\n\n- Following the ONCAT instructions above, once located in the data download page, we can see other options for data download in the page, including using `SSH`, `FileZilla` or `CyberDuck`."}
{"text": "Scope: Data access\nCategory: Data Flow\nQ: What's the process for downloading data from SNS instruments?\nA: There are multiple options, as listed below,\n\n- Go to `https://nova.ornl.gov/`, log in with UCAMS/XCAMS account. Go to `Powder Diffraction` and launch the `PDiffVis` tool. With the tool, you can visualize the data and download them straightforwardly.\n\n- Go to `https://oncat.ornl.gov`, log in with UCAMS/XCAMS account, click on `Browse`, select the instrument, go to `Experiments`, select the IPTS, click on `Download` to go to the data download page. The option on the top will be to download data from the web.\n\n- Following the ONCAT instructions above, once located in the data download page, we can see other options for data download in the page, including using `SSH`, `FileZilla` or `CyberDuck`."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I have my data processed on-the-fly while the data collection is still ongoing?\nA: Depending on which powder diffraction instruments you are on, you may or may not have the capability to process the live streamed data while the collection is still ongoing. POWGEN now has already got this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing. On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. For HB-2A and HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Is live data processing available during data collection?\nA: Depending on which powder diffraction instruments you are on, you may or may not have the capability to process the live streamed data while the collection is still ongoing. POWGEN now has already got this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing. On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. For HB-2A and HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I monitor my data in real-time while collecting?\nA: Depending on which powder diffraction instruments you are on, you may or may not have the capability to process the live streamed data while the collection is still ongoing. POWGEN now has already got this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing. On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. For HB-2A and HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Is on-the-fly data processing supported?\nA: Depending on which powder diffraction instruments you are on, you may or may not have the capability to process the live streamed data while the collection is still ongoing. POWGEN now has already got this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing. On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. For HB-2A and HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I see processed data while my experiment is running?\nA: Depending on which powder diffraction instruments you are on, you may or may not have the capability to process the live streamed data while the collection is still ongoing. POWGEN now has already got this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing. On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. For HB-2A and HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Tell me about live data processing capabilities.\nA: Depending on which powder diffraction instruments you are on, you may or may not have the capability to process the live streamed data while the collection is still ongoing. POWGEN now has already got this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing. On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. For HB-2A and HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: What instruments support live data processing?\nA: Depending on which powder diffraction instruments you are on, you may or may not have the capability to process the live streamed data while the collection is still ongoing. POWGEN now has already got this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing. On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. For HB-2A and HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I have my NOMAD data processed on-the-fly while the data collection is still ongoing?\nA: On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Does NOMAD support live data processing?\nA: On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I monitor NOMAD data in real-time?\nA: On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Is on-the-fly processing available on NOMAD?\nA: On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Tell me about NOMAD live data processing.\nA: On NOMAD, the live data processing capability is also available. However, the processing is only as simple as align (in d-space) and focus (summing up patterns from all pixels). The purpose here is for caching and performance boosting for the data reduction at later stage when the data are available on hard drive. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I have my POWGEN data processed on-the-fly while the data collection is still ongoing?\nA: POWGEN does have this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Does POWGEN support live data processing?\nA: POWGEN does have this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I monitor POWGEN data in real-time?\nA: POWGEN does have this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Is on-the-fly processing available on POWGEN?\nA: POWGEN does have this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Tell me about POWGEN live data processing.\nA: POWGEN does have this capability and you can go to https://monitor.sns.gov/dasmon/pg3/, click on the ongoing IPTS from the top of the page and then click on the run on the very top (i.e., the current run). There you can see the processed live streamed data updated periodically. By default, the live processing engine takes 30 seconds of live streamed data as a chunk, process it and accumulate the data as the collection is ongoing."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I have my HB-2A data processed on-the-fly while the data collection is still ongoing?\nA: For HB-2A, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Does HB-2A support live data processing?\nA: For HB-2A, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I monitor HB-2A data in real-time?\nA: For HB-2A, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Is on-the-fly processing available on HB-2A?\nA: For HB-2A, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Tell me about HB-2A live data processing.\nA: For HB-2A, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I have my HB-2C data processed on-the-fly while the data collection is still ongoing?\nA: For HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Does HB-2C support live data processing?\nA: For HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Can I monitor HB-2C data in real-time?\nA: For HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Is on-the-fly processing available on HB-2C?\nA: For HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data processing\nCategory: Live processing\nQ: Tell me about HB-2C live data processing.\nA: For HB-2C, currently we don't have the live data processing capability enabled."}
{"text": "Scope: Data visualization\nCategory: Data analysis\nQ: I want to visualize my data quickly, show me some options.\nA: There are multiple options I would recommend,\n\n- If your data are on ORNL Analysis (where all your reduced data are supposed to live in the first place), go to `https://nova.ornl.gov`, log in with UCAMS/XCAMS, and straightforward data visualization is provided for both plain-text data presented in the column form or Bragg diffraction data in the GSAS format.\n\n- If your data are on your local machine, you may want to stop by `https://addie.ornl.gov/plotter` to simply upload your data and visualize. Only plain text data in the column form are accepted. The website supports the visualization of tens of files at a time."}
{"text": "Scope: Data visualization\nCategory: Data analysis\nQ: How can I quickly visualize my data?\nA: There are multiple options I would recommend,\n\n- If your data are on ORNL Analysis (where all your reduced data are supposed to live in the first place), go to `https://nova.ornl.gov`, log in with UCAMS/XCAMS, and straightforward data visualization is provided for both plain-text data presented in the column form or Bragg diffraction data in the GSAS format.\n\n- If your data are on your local machine, you may want to stop by `https://addie.ornl.gov/plotter` to simply upload your data and visualize. Only plain text data in the column form are accepted. The website supports the visualization of tens of files at a time."}
{"text": "Scope: Data visualization\nCategory: Data analysis\nQ: What are the options for data visualization?\nA: There are multiple options I would recommend,\n\n- If your data are on ORNL Analysis (where all your reduced data are supposed to live in the first place), go to `https://nova.ornl.gov`, log in with UCAMS/XCAMS, and straightforward data visualization is provided for both plain-text data presented in the column form or Bragg diffraction data in the GSAS format.\n\n- If your data are on your local machine, you may want to stop by `https://addie.ornl.gov/plotter` to simply upload your data and visualize. Only plain text data in the column form are accepted. The website supports the visualization of tens of files at a time."}
{"text": "Scope: Data visualization\nCategory: Data analysis\nQ: I need to plot my data, what tools are available?\nA: There are multiple options I would recommend,\n\n- If your data are on ORNL Analysis (where all your reduced data are supposed to live in the first place), go to `https://nova.ornl.gov`, log in with UCAMS/XCAMS, and straightforward data visualization is provided for both plain-text data presented in the column form or Bragg diffraction data in the GSAS format.\n\n- If your data are on your local machine, you may want to stop by `https://addie.ornl.gov/plotter` to simply upload your data and visualize. Only plain text data in the column form are accepted. The website supports the visualization of tens of files at a time."}
{"text": "Scope: Data visualization\nCategory: Data analysis\nQ: Tell me about data visualization options.\nA: There are multiple options I would recommend,\n\n- If your data are on ORNL Analysis (where all your reduced data are supposed to live in the first place), go to `https://nova.ornl.gov`, log in with UCAMS/XCAMS, and straightforward data visualization is provided for both plain-text data presented in the column form or Bragg diffraction data in the GSAS format.\n\n- If your data are on your local machine, you may want to stop by `https://addie.ornl.gov/plotter` to simply upload your data and visualize. Only plain text data in the column form are accepted. The website supports the visualization of tens of files at a time."}
{"text": "Scope: Data visualization\nCategory: Data analysis\nQ: How do I visualize my powder diffraction data?\nA: There are multiple options I would recommend,\n\n- If your data are on ORNL Analysis (where all your reduced data are supposed to live in the first place), go to `https://nova.ornl.gov`, log in with UCAMS/XCAMS, and straightforward data visualization is provided for both plain-text data presented in the column form or Bragg diffraction data in the GSAS format.\n\n- If your data are on your local machine, you may want to stop by `https://addie.ornl.gov/plotter` to simply upload your data and visualize. Only plain text data in the column form are accepted. The website supports the visualization of tens of files at a time."}
{"text": "Scope: Data visualization\nCategory: Data analysis\nQ: What tools can I use to plot my data?\nA: There are multiple options I would recommend,\n\n- If your data are on ORNL Analysis (where all your reduced data are supposed to live in the first place), go to `https://nova.ornl.gov`, log in with UCAMS/XCAMS, and straightforward data visualization is provided for both plain-text data presented in the column form or Bragg diffraction data in the GSAS format.\n\n- If your data are on your local machine, you may want to stop by `https://addie.ornl.gov/plotter` to simply upload your data and visualize. Only plain text data in the column form are accepted. The website supports the visualization of tens of files at a time."}
{"text": "Scope: Data processing\nCategory: Absorption correction\nQ: My samples contain neutron-absorbing elements, is this considered as a problem?\nA: That depends on how strongly your sample is absorbing neutrons. For elements that absorb neutrons in 'mild' way (e.g., Li), our absorption correction routine implemented into the time-of-flight powder instruments (POWGEN and NOMAD) can handle it. Detailed information can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_abs_ms.html`. If the absorption is too strong, e.g., Cd, Dy, B, etc., though, the absorption correction can still try to do it, strong uncertainty is expected."}
{"text": "Scope: Data processing\nCategory: Absorption correction\nQ: I have neutron-absorbing elements in my sample, what should I do?\nA: That depends on how strongly your sample is absorbing neutrons. For elements that absorb neutrons in 'mild' way (e.g., Li), our absorption correction routine implemented into the time-of-flight powder instruments (POWGEN and NOMAD) can handle it. Detailed information can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_abs_ms.html`. If the absorption is too strong, e.g., Cd, Dy, B, etc., though, the absorption correction can still try to do it, strong uncertainty is expected."}
{"text": "Scope: Data processing\nCategory: Absorption correction\nQ: Can you handle samples with strong neutron absorption?\nA: That depends on how strongly your sample is absorbing neutrons. For elements that absorb neutrons in 'mild' way (e.g., Li), our absorption correction routine implemented into the time-of-flight powder instruments (POWGEN and NOMAD) can handle it. Detailed information can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_abs_ms.html`. If the absorption is too strong, e.g., Cd, Dy, B, etc., though, the absorption correction can still try to do it, strong uncertainty is expected."}
{"text": "Scope: Data processing\nCategory: Absorption correction\nQ: My sample contains Cd/B/Dy, is this a problem?\nA: That depends on how strongly your sample is absorbing neutrons. For elements that absorb neutrons in 'mild' way (e.g., Li), our absorption correction routine implemented into the time-of-flight powder instruments (POWGEN and NOMAD) can handle it. Detailed information can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_abs_ms.html`. If the absorption is too strong, e.g., Cd, Dy, B, etc., though, the absorption correction can still try to do it, strong uncertainty is expected."}
{"text": "Scope: Data processing\nCategory: Absorption correction\nQ: Tell me about neutron absorption correction.\nA: That depends on how strongly your sample is absorbing neutrons. For elements that absorb neutrons in 'mild' way (e.g., Li), our absorption correction routine implemented into the time-of-flight powder instruments (POWGEN and NOMAD) can handle it. Detailed information can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_abs_ms.html`. If the absorption is too strong, e.g., Cd, Dy, B, etc., though, the absorption correction can still try to do it, strong uncertainty is expected."}
{"text": "Scope: Data processing\nCategory: Absorption correction\nQ: How do you handle neutron-absorbing samples?\nA: That depends on how strongly your sample is absorbing neutrons. For elements that absorb neutrons in 'mild' way (e.g., Li), our absorption correction routine implemented into the time-of-flight powder instruments (POWGEN and NOMAD) can handle it. Detailed information can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_abs_ms.html`. If the absorption is too strong, e.g., Cd, Dy, B, etc., though, the absorption correction can still try to do it, strong uncertainty is expected."}
{"text": "Scope: Data processing\nCategory: Absorption correction\nQ: Is neutron absorption correction available?\nA: That depends on how strongly your sample is absorbing neutrons. For elements that absorb neutrons in 'mild' way (e.g., Li), our absorption correction routine implemented into the time-of-flight powder instruments (POWGEN and NOMAD) can handle it. Detailed information can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_abs_ms.html`. If the absorption is too strong, e.g., Cd, Dy, B, etc., though, the absorption correction can still try to do it, strong uncertainty is expected."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: I want to process my total scattering data, how can I do it?\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: How do I process total scattering data?\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: Tell me about total scattering data reduction.\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: What's the process for reducing total scattering data?\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: I need to reduce my total scattering data.\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: How can I process my PDF data?\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: What tools are available for total scattering data processing?\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: I want to process my total scattering data, do we have any resources or references?\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: What resources are available for total scattering data processing?\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: Where can I find documentation for total scattering data reduction?\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: Tell me about resources for processing total scattering data.\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Total scattering\nQ: I need references for total scattering data processing.\nA: That depends on what you try to do. For the raw data processing, i.e., from neutron counting to reduced data. in most cases, you don't need to worry about that, as it will be taken care of, by the autoreduction routines deployed on powder diffractometers at SNS (POWGEN & NOMAD). A cyclic approach is implemented in the routine to loop over the packing fraction until the level at high-Q reaches the self-scattering level for the reduced structure factor. In those complicated cases (e.g., sample amount being too small) or any inaccuracy of the approximation in the data processing (e.g., the absorption correction), a post-processing may be needed. If such 'post-processing' is what you mean by 'process', I can share some more details.\n\n- At SNS, the two powder diffractometers NOMAD and POWGEN are using a consistent routine for producing the total scattering data, namely `MantidTotalScattering` (MTS). The documentation can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`. Through the autoreduction, input files for running MTS would be saved. Further processing can be done by tweaking parameters on top of those automatically generated input files and re-running the MTS reduction routine, as detailed below.\n\n- For `NOMAD`, the input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation.\n\n- For `POWGEN`, the input files generated through autoreduction can be found at, `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/MTSRed/Input`.\n\n- Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` are expected to be changed for further processing. After making the changes, the new input file can be run via `mts /path/to/<input>.json` on a terminal on the ORNL Analysis cluster, where `<input>.json` (without bracket) refers to the name of the new input file (in JSON format).\n\n- Some further post-processing may be needed, e.g., tweaking the data scaling or Fourier filtering. Multiple tools are available for such a purpose. The `StoG` utility in the `RMCProfile` package can be used and some documentation can be found here, `https://rmcprofile.ornl.gov/data-pre-processing-for-rmcprofile/`. The Python version of the program can also be used and some instructions can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/ts_pp.html`.\n\n- If the data are noisy, you can also consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Data denoising\nQ: My total scattering data are noisy, do we have any routines for data denoising?\nA: If the data are noisy, you can consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Data denoising\nQ: How can I denoise my total scattering data?\nA: If the data are noisy, you can consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Data denoising\nQ: My data are noisy, what can I do?\nA: If the data are noisy, you can consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Data denoising\nQ: Tell me about data denoising options.\nA: If the data are noisy, you can consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Data denoising\nQ: I need to reduce noise in my total scattering data.\nA: If the data are noisy, you can consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Data denoising\nQ: What tools are available for denoising total scattering data?\nA: If the data are noisy, you can consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
{"text": "Scope: Data processing\nCategory: Data denoising\nQ: How do I improve the quality of noisy data?\nA: If the data are noisy, you can consider using the `pystog_ck` utility available on the ORNL Analysis cluster for processing the data in a chunk-by-chunk manner to encode the data denoising. Documentation can be found here, `https://powder.ornl.gov/data_tools/general.html#pystog-ck`. Source codes can be found here, `https://github.com/Kvieta1990/neutron_ts_tools/tree/main/pystog_ck/pystog_ck`."}
