I have the following pairs of Q and A and I want to prepare the training data using a similar routine as for the current script.

---

Q: I need to rerun the processing for my POWGEN data. Give me some information.
A: On POWGEN, the autoreduction routine is controlled through parameter files located at dedicated places. Instructions for populating those configuration files can be found here, `https://powder.ornl.gov/auto_reduce/powgen_auto.html`. There are two places where we can change the parameters. One is in the POWGEN shared folder (i.e., the `central` config, as detailed in the link) and the other is in the user experiment directory. Users are supposed to only touch the user-level configuration file. The logic about which one takes priority is set in the `central` config for which the information can be found in the link. The user-level config file can be found at `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/auto_params.json`. Once changes are made to the configuration file (either the `central` one or the user-level one), data can be processed in a batch, with the `all_pg3` utility, like `all_pg3 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. The command is accessible in terminal on the ORNL Analysis cluster. Processed data will be saved into `/SNS/PG3/IPTS-xxxxx/shared/autoreduce`.

With the autoreduction routine, an `XML` file would also be saved, which can be later imported into the GUI for POWGEN data reduction inside `Mantid Workbench`. The `XML` file would be saved into `/SNS/PG3/IPTS-xxxxx/shared/autoreduce`. To use it, log in the ORNL Analysis cluster (see here, analysis.sns.gov), open a terminal and type `mantidworkbench`. Inside the interface, go to `Interfaces` in the menu, and further go to `Diffraction` => `Powder Diffraction Reduction` to launch the GUI for POWGEN data processing. There, we go to `File` => `Open`, and look for the saved `XML` file to load it into the GUI. Then we can make changes as needed and click on the `Reduce` button to process the data.

---

Q: I need to process my POWGEN data. Give me some information.
A: Normally, the POWGEN autoreduction routine will take care of the data processing and reduced data can be found at `/SNS/PG3/IPTS-xxxxx/shared/autoreduce`. In case of parameters tweaking and a re-processing is needed, I can provide some information about it. On POWGEN, the autoreduction routine is controlled through parameter files located at dedicated places. Instructions for populating those configuration files can be found here, `https://powder.ornl.gov/auto_reduce/powgen_auto.html`. There are two places where we can change the parameters. One is in the POWGEN shared folder (i.e., the `central` config, as detailed in the link) and the other is in the user experiment directory. Users are supposed to only touch the user-level configuration file. The logic about which one takes priority is set in the `central` config for which the information can be found in the link. The user-level config file can be found at `/SNS/PG3/IPTS-xxxxx/shared/autoreduce/auto_params.json`. Once changes are made to the configuration file (either the `central` one or the user-level one), data can be processed in a batch, with the `all_pg3` utility, like `all_pg3 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. The command is accessible in terminal on the ORNL Analysis cluster. Processed data will be saved into `/SNS/PG3/IPTS-xxxxx/shared/autoreduce`.

With the autoreduction routine, an `XML` file would also be saved, which can be later imported into the GUI for POWGEN data reduction inside `Mantid Workbench`. The `XML` file would be saved into `/SNS/PG3/IPTS-xxxxx/shared/autoreduce`. To use it, log in the ORNL Analysis cluster (see here, analysis.sns.gov), open a terminal and type `mantidworkbench`. Inside the interface, go to `Interfaces` in the menu, and further go to `Diffraction` => `Powder Diffraction Reduction` to launch the GUI for POWGEN data processing. There, we go to `File` => `Open`, and look for the saved `XML` file to load it into the GUI. Then we can make changes as needed and click on the `Reduce` button to process the data.

---

Q: I need to rerun the processing for my NOMAD data. Give me some information.
A: On NOMAD, there are two levels of parameter configuration for the autoreduction. In the NOMAD shared directory, there is a configuration file in the JSON format (`/SNS/NOM/shared/autoreduce/auto_config.json`) containing some common parameters shared across all measurements. Only NOMAD team members have the permission of editing and in fact, parameters included in the file are not expected to be changed frequently. Characterization runs information (typically, calibration file, container and empty instrument background, and vanadium normalization) is contained in `/SNS/NOM/shared/autoreduce/auto_exp.csv`. In case any of the two files are updated and we need to rerun the autoreduction, we can log into the ORNL Analysis (see here, analysis.sns.gov) and run the command like `all_nom 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. The command is accessible in terminal on the ORNL Analysis cluster. Processed data will be saved into `/SNS/NOM/IPTS-xxxxx/shared/autoreduce`. More detailed information can be found here, `https://powder.ornl.gov/auto_reduce/nomad_auto.html`.

The other level of configuration is measurement-specific, in which case individual measurements have to be processed individually based on the input JSON files created through the autoreduction routine. Those input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation. Depending on which mode we want to go with (i.e., `multi_banks` or `single_bank`), we can go into the corresponding directory, make changes to the intended input file and run the data processing via `mts <input>.json` from a terminal (for sure, you need to run something like `cd /SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` to go to the directory first, in the terminal). `<input>.json` (without bracket) represents the input JSON file that you want to run. Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` may be expected to be changed for the re-processing. Detailed information about the input file for running `MantidTotalScattering` (what the command `mts` is short for) can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`.

For autoreduction and the processing with `all_nom`, output of reduced data can be found at something like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce`. For manual processing with `mts`, the output directory can be specified explicitly in the input JSON file.

---

Q: I need to process my NOMAD data. Give me some information.
A: Normally, the NOMAD autoreduction routine will take care of the data processing and reduced data can be found at `/SNS/NOM/IPTS-xxxxx/shared/autoreduce`. In case of parameters tweaking and a re-processing is needed, I can provide some information about it. On NOMAD, there are two levels of parameter configuration for the autoreduction. In the NOMAD shared directory, there is a configuration file in the JSON format (`/SNS/NOM/shared/autoreduce/auto_config.json`) containing some common parameters shared across all measurements. Only NOMAD team members have the permission of editing and in fact, parameters included in the file are not expected to be changed frequently. Characterization runs information (typically, calibration file, container and empty instrument background, and vanadium normalization) is contained in `/SNS/NOM/shared/autoreduce/auto_exp.csv`. In case any of the two files are updated and we need to rerun the autoreduction, we can log into the ORNL Analysis (see here, analysis.sns.gov) and run the command like `all_nom 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. The command is accessible in terminal on the ORNL Analysis cluster. Processed data will be saved into `/SNS/NOM/IPTS-xxxxx/shared/autoreduce`. More detailed information can be found here, `https://powder.ornl.gov/auto_reduce/nomad_auto.html`.

The other level of configuration is measurement-specific, in which case individual measurements have to be processed individually based on the input JSON files created through the autoreduction routine. Those input files can be found in locations like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/multi_banks_summed/Input` or `/SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` on the ORNL Analysis cluster. The `multi_banks` mode here means the data would be grouped according to those physical banks of detectors. The `single_bank` mode means data from all detectors would be merged into a single pattern. The `summed` here in the path means all files corresponding to the same measurement are summed together. This is specific to the convention followed on NOMAD for the data collection -- usually, a long data collection would be done through multiple chunks of collection. For example, a total 8 C proton charge accumulation would be collected through 4 chunks, with each chunk contributing 2 C proton charge accumulation. Depending on which mode we want to go with (i.e., `multi_banks` or `single_bank`), we can go into the corresponding directory, make changes to the intended input file and run the data processing via `mts <input>.json` from a terminal (for sure, you need to run something like `cd /SNS/NOM/IPTS-xxxxx/shared/autoreduce/single_bank_summed/Input` to go to the directory first, in the terminal). `<input>.json` (without bracket) represents the input JSON file that you want to run. Usually, parameters like `Scale` in the `Background` entry of `Sample` or the `Type` entry in `AbsorptionCorrection` may be expected to be changed for the re-processing. Detailed information about the input file for running `MantidTotalScattering` (what the command `mts` is short for) can be found here, `https://powder.ornl.gov/total_scattering/data_reduction/mts_doc.html`.

For autoreduction and the processing with `all_nom`, output of reduced data can be found at something like `/SNS/NOM/IPTS-xxxxx/shared/autoreduce`. For manual processing with `mts`, the output directory can be specified explicitly in the input JSON file.

---

Q: I need to rerun the processing for my HB2C data. Give me some information.
A: On HB2C, autoreduction service is available to process the data automatically. Processed data would be saved into location like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`. Multiple folders are included as the output, including `CSV`, `DAT` and `XYE` -- they are fundamentally the same reduced data but with different output formats. Configuration for the autoreduction is contained in parameter files located at dedicated places. In the HB2C shared location (`/HFIR/HB2C/shared/autoreduce`), the configuration file `auto_config.json` can be changed according to the parameters tweaking need. The `auto_exp.csv` file contains the information about the normalization runs to be used for the data processing. Only the HB2C team members have the permission to change these two files. On the user-level, if changes are to be made, we can go to the IPTS directory, like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce` and edit the file `auto_params.json`. The logic here is, if the user-level configuration file exists, it will take the priority in running the data processing.

To run the processing for a series of measurements, we can log into the ORNL Analysis cluster (see here, analysis.sns.gov), open a terminal and run `all_wpd 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. Processed data will be saved into something like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`.

---

Q: I need to process my HB2C data. Give me some information.
A: Normally, the HB2C autoreduction routine will take care of the data processing and reduced data can be found at `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`. In case of parameters tweaking and a re-processing is needed, I can provide some information about it. With the autoreduction routine for HB2C, multiple folders are included as the output, including `CSV`, `DAT` and `XYE` -- they are fundamentally the same reduced data but with different output formats. Configuration for the autoreduction is contained in parameter files located at dedicated places. In the HB2C shared location (`/HFIR/HB2C/shared/autoreduce`), the configuration file `auto_config.json` can be changed according to the parameters tweaking need. The `auto_exp.csv` file contains the information about the normalization runs to be used for the data processing. Only the HB2C team members have the permission to change these two files. On the user-level, if changes are to be made, we can go to the IPTS directory, like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce` and edit the file `auto_params.json`. The logic here is, if the user-level configuration file exists, it will take the priority in running the data processing.

To run the processing for a series of measurements, we can log into the ORNL Analysis cluster (see here, analysis.sns.gov), open a terminal and run `all_wpd 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. Processed data will be saved into something like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`.

---

Q: I need to rerun the processing for my WAND^2 data. Give me some information.
A: On WAND^2, autoreduction service is available to process the data automatically. Processed data would be saved into location like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`. Multiple folders are included as the output, including `CSV`, `DAT` and `XYE` -- they are fundamentally the same reduced data but with different output formats. Configuration for the autoreduction is contained in parameter files located at dedicated places. In the WAND^2 shared location (`/HFIR/HB2C/shared/autoreduce`), the configuration file `auto_config.json` can be changed according to the parameters tweaking need. The `auto_exp.csv` file contains the information about the normalization runs to be used for the data processing. Only the WAND^2 team members have the permission to change these two files. On the user-level, if changes are to be made, we can go to the IPTS directory, like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce` and edit the file `auto_params.json`. The logic here is, if the user-level configuration file exists, it will take the priority in running the data processing.

To run the processing for a series of measurements, we can log into the ORNL Analysis cluster (see here, analysis.sns.gov), open a terminal and run `all_wpd 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. Processed data will be saved into something like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`.

---

Q: I need to process my WAND^2 data. Give me some information.
A: Normally, the WAND^2 autoreduction routine will take care of the data processing and reduced data can be found at `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`. In case of parameters tweaking and a re-processing is needed, I can provide some information about it. With the autoreduction routine for WAND^2, multiple folders are included as the output, including `CSV`, `DAT` and `XYE` -- they are fundamentally the same reduced data but with different output formats. Configuration for the autoreduction is contained in parameter files located at dedicated places. In the WAND^2 shared location (`/HFIR/HB2C/shared/autoreduce`), the configuration file `auto_config.json` can be changed according to the parameters tweaking need. The `auto_exp.csv` file contains the information about the normalization runs to be used for the data processing. Only the WAND^2 team members have the permission to change these two files. On the user-level, if changes are to be made, we can go to the IPTS directory, like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce` and edit the file `auto_params.json`. The logic here is, if the user-level configuration file exists, it will take the priority in running the data processing.

To run the processing for a series of measurements, we can log into the ORNL Analysis cluster (see here, analysis.sns.gov), open a terminal and run `all_wpd 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. Processed data will be saved into something like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`.

---

Q: I need to rerun the processing for my WAND data. Give me some information.
A: On WAND, autoreduction service is available to process the data automatically. Processed data would be saved into location like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`. Multiple folders are included as the output, including `CSV`, `DAT` and `XYE` -- they are fundamentally the same reduced data but with different output formats. Configuration for the autoreduction is contained in parameter files located at dedicated places. In the WAND shared location (`/HFIR/HB2C/shared/autoreduce`), the configuration file `auto_config.json` can be changed according to the parameters tweaking need. The `auto_exp.csv` file contains the information about the normalization runs to be used for the data processing. Only the WAND team members have the permission to change these two files. On the user-level, if changes are to be made, we can go to the IPTS directory, like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce` and edit the file `auto_params.json`. The logic here is, if the user-level configuration file exists, it will take the priority in running the data processing.

To run the processing for a series of measurements, we can log into the ORNL Analysis cluster (see here, analysis.sns.gov), open a terminal and run `all_wpd 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. Processed data will be saved into something like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`.

---

Q: I need to process my WAND data. Give me some information.
A: Normally, the WAND autoreduction routine will take care of the data processing and reduced data can be found at `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`. In case of parameters tweaking and a re-processing is needed, I can provide some information about it. With the autoreduction routine for WAND, multiple folders are included as the output, including `CSV`, `DAT` and `XYE` -- they are fundamentally the same reduced data but with different output formats. Configuration for the autoreduction is contained in parameter files located at dedicated places. In the WAND shared location (`/HFIR/HB2C/shared/autoreduce`), the configuration file `auto_config.json` can be changed according to the parameters tweaking need. The `auto_exp.csv` file contains the information about the normalization runs to be used for the data processing. Only the WAND team members have the permission to change these two files. On the user-level, if changes are to be made, we can go to the IPTS directory, like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce` and edit the file `auto_params.json`. The logic here is, if the user-level configuration file exists, it will take the priority in running the data processing.

To run the processing for a series of measurements, we can log into the ORNL Analysis cluster (see here, analysis.sns.gov), open a terminal and run `all_wpd 12345 "10000-10050"`, where `12345` represents the IPTS number and "10000-10050" represents a range of run numbers within that IPTS. Processed data will be saved into something like `/HFIR/HB2C/IPTS-xxxxx/shared/autoreduce`.

---

Q: I need to rerun the processing for my HB2A data. Give me some information.
A: On HB2A, autoreduction service is available to process the data automatically. Processed data would be saved into location like `/HFIR/HB2A/IPTS-xxxxx/shared/autoreduce`. In most cases, autoreduction should handle the data processing seamlessly. In some rare cases when the autoreduction is not giving reasonable output, please get in touch with the team (https://neutrons.ornl.gov/powder/team).

---

Q: I need to process my HB2A data. Give me some information.
A: On HB2A, autoreduction service is available to process the data automatically. Processed data would be saved into location like `/HFIR/HB2A/IPTS-xxxxx/shared/autoreduce`. In most cases, autoreduction should handle the data processing seamlessly. In some rare cases when the autoreduction is not giving reasonable output, please get in touch with the team (https://neutrons.ornl.gov/powder/team).

---